{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from shapely.geometry import Polygon\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from skimage import transform\n",
    "import matplotlib.image as mpimg\n",
    "from torch.utils.data import DataLoader\n",
    "from floortrans.models import get_model\n",
    "from floortrans.loaders import FloorplanSVG, DictToTensor, Compose, RotateNTurns\n",
    "from floortrans.plotting import segmentation_plot, polygons_to_image, draw_junction_from_dict, discrete_cmap\n",
    "#discrete_cmap()\n",
    "from floortrans.post_prosessing import split_prediction, get_polygons, split_validation\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rot = RotateNTurns() #\n",
    "\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\", \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "icon_classes = [\"No Icon\", \"Window\", \"Door\", \"Closet\", \"Electrical Applience\" ,\"Toilet\", \"Sink\", \"Sauna Bench\", \"Fire Place\", \"Bathtub\", \"Chimney\"]\n",
    "room_classes.append(\"Door\")\n",
    "\n",
    "data_folder = 'data/cubicasa5k/'\n",
    "data_file = 'test.txt'\n",
    "normal_set = FloorplanSVG(data_folder, data_file, format='txt', original_size=True)\n",
    "data_loader = DataLoader(normal_set, batch_size=1, num_workers=0)\n",
    "data_iter = iter(data_loader)\n",
    "\n",
    "# Setup Model\n",
    "model = get_model('hg_furukawa_original', 51)\n",
    "n_classes = 44\n",
    "split = [21, 12, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_class(rooms, CLASS: int):\n",
    "    template = np.zeros_like(rooms)\n",
    "    rows, cols = np.where(rooms == CLASS)\n",
    "    template[rows, cols] = 1\n",
    "    return template\n",
    "    \n",
    "bad=[0, 1, 2, 8, 11]\n",
    "good=[3,4,5,6,7,9,10,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: a floorplan in numpy, a list of room types in ints\n",
    "#output: room_contours: a dictionary where each key is a roomtype in int, and the value is a list of the contours where the floorplan has that room\n",
    "#room_contours[12] : the list of contours where the roomtype is a door\n",
    "#nodes: a dictionary where each key is a roomtype in int and the value is  like a list of array([522.5, 219. ])\n",
    "#this is a list of nd array\n",
    "def vis_nodes(img, significant_nodes):\n",
    "    # signficant nodes exclude rooms we don't care about\n",
    "    nodes, room_contours= {},{}\n",
    "    \n",
    "    for c in significant_nodes: # c represents the rooms that we care about, in integer representation of rooms_list\n",
    "        nodes[c], room_contours[c]=[], []\n",
    "        t = isolate_class(img, c) # this returns np representation of the floorplan, with the pixels being 1 where it is a specific room, 0 elsewhere\n",
    "        #CHANGED BELOW TO CHANGE_APPROX_SIMPLE\n",
    "        contours, _ = cv2.findContours(t.astype(np.uint8), mode=cv2.RETR_EXTERNAL, method=cv2.CHAIN_APPROX_SIMPLE) #retr_external means ignore internal gaps in images. if we have donut it becomes a circle. chain_approx_none means that all boundary points are stored. chain_approx_simple removes redundant points\n",
    "        \n",
    "        for s in contours:\n",
    "            room_contours[c].append(s) # dictionary is {room_type_as_int:[contour1, contour2...]}\n",
    "            nodes[c].append(np.squeeze(np.array(s), 1).mean(0)) # dictionary is {room_type_as_int: [(center in horizontal of contour, center in vertical of contour), (x,y for contour 2...)]}\n",
    "\n",
    "    return(room_contours, room_contours[12], nodes) # room contours, door contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each node attribute has type and area\n",
    "pass in contour,polygon and get area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test is the modified dataset from when we defined some undefined rooms using the count of icons using a classification model\n",
    "\n",
    "# change this line for each dataset. currently creating embeddings from test\n",
    "with open(\"my_data/test_modified_1.pkl\", 'rb') as f:\n",
    "    test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings,Y=None, None # return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, floorplan in test.items():\n",
    "    if 11 not in set(floorplan.flatten()): # 11 represents undefined rooms. we don't care about floorplans that contain this\n",
    "        icons=normal_set[index]['label'][1].numpy() #np representations of icons on the floorplan\n",
    "        row, column=np.where(icons==2) #finding doors on this floorplan\n",
    "        floorplan[row, column]=12 #modifying the floorplan to include doors as a room\n",
    "        rooms, doors, nodes=vis_nodes(floorplan, good)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: [array([522.5, 219. ])],\n",
       " 4: [array([478.    , 462.4375])],\n",
       " 5: [array([111.5, 379. ]),\n",
       "  array([394.5, 195.5]),\n",
       "  array([254.5, 157. ]),\n",
       "  array([111.5, 178. ])],\n",
       " 6: [array([111.5, 291. ]),\n",
       "  array([683.5, 282.5]),\n",
       "  array([758. , 270.5]),\n",
       "  array([617.5, 254.5])],\n",
       " 7: [array([217., 400.]),\n",
       "  array([236.57142857, 292.14285714]),\n",
       "  array([686.9, 206. ])],\n",
       " 9: [array([300., 252.])],\n",
       " 10: [],\n",
       " 12: [array([205. , 435.5]),\n",
       "  array([212.5, 368. ]),\n",
       "  array([187.5, 368. ]),\n",
       "  array([688. , 361.5]),\n",
       "  array([171., 347.]),\n",
       "  array([726., 302.]),\n",
       "  array([356.5, 290.5]),\n",
       "  array([171. , 294.5]),\n",
       "  array([337.5, 251.5]),\n",
       "  array([171., 235.]),\n",
       "  array([746. , 218.5]),\n",
       "  array([623. , 218.5]),\n",
       "  array([215.5, 213.5]),\n",
       "  array([793., 187.]),\n",
       "  array([594., 187.]),\n",
       "  array([573., 139.])]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nodes[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "127d3802e688a92476c27dfe182bc5f7b571b121c08898fa2acccdb54b120db2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
